{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "important-platform",
   "metadata": {},
   "source": [
    "## Second Assignment - Working with Named Entities\n",
    "\n",
    "- **Student**: Farina Matteo  \n",
    "- **Mat. Number**: 221252  \n",
    "- **email**: [matteo.farina-1@studenti.unitn.it](mailto:matteo.farina-1@studenti.unitn.it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-survival",
   "metadata": {},
   "source": [
    "### Content  \n",
    "This notebook contains the code for the second assignment of the course. Additionally, detailed descriptions are provided right before each code cell, so that this notebook can play the role of the report too.  \n",
    "  \n",
    "The structure of the notebook consists of several sections, briefly listed below.\n",
    "- [**Requirements**](#Requirements): section for external libraries and packages.\n",
    "- [**Task 1**](#Task-1) Evaluate spaCy NER on CoNLL 2003 data.\n",
    "    - [**Task 1.1**](#Task-1.1) report token-level performance (per class and total) and accuracy\n",
    "    - [**Task 1.2**](#Task-1.2) report chunk-level metrics (precision, recall and f1-score) of recognizing all the named entities in a chunk per class and total\n",
    "- [**Task 2**](#Task-2) Grouping of Entities and analysis in terms of most frequent NER combinations.\n",
    "- [**Task 3**](#Task-3) Extending Entity Spans with compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-parliament",
   "metadata": {},
   "source": [
    "### Requirements  \n",
    "\n",
    "In here, python dependencies and actions to be performed in order to have your system up and running for this notebook are illustrated. Please note that this notebook has been tested under the **Python 3.9** interpreter inside an Anaconda virtual environment. External libraries have been installed with **pip**, version 21.0.1.  \n",
    "\n",
    "The main library that is used throughout the notebook is `spaCy`. To install it, run:  \n",
    "- `pip install spacy`  \n",
    "\n",
    "Other than that, in this notebook spaCy is coupled with its primary english language pipeline (small one):  \n",
    "- `python -m spacy download en_core_web_sm`  \n",
    "\n",
    "For evaluation purposes, `scikit-learn` is used. To install it, run:  \n",
    "- `pip install scikit-learn`  \n",
    "  \n",
    "Finally, `pandas` is required to render beatiful tables to visualize our data:  \n",
    "- `pip install pandas`  \n",
    "\n",
    "#### Shortcut  \n",
    "If you prefer to avoid individual install commands, a requirements file has been included in the repo.\n",
    "Note that spaCy's english language must be manually installed anyway. You can install everything by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fbe5ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 257 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: argon2-cffi==20.1.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (20.1.0)\n",
      "Requirement already satisfied: async-generator==1.10 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.10)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (20.3.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: bleach==3.3.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: blis==0.7.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.7.4)\n",
      "Requirement already satisfied: catalogue==2.0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: certifi==2020.12.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (2020.12.5)\n",
      "Requirement already satisfied: cffi==1.14.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.14.5)\n",
      "Requirement already satisfied: chardet==4.0.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.0.0)\n",
      "Requirement already satisfied: click==7.1.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (7.1.2)\n",
      "Requirement already satisfied: cymem==2.0.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (2.0.5)\n",
      "Requirement already satisfied: decorator==5.0.7 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (5.0.7)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints==0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.3)\n",
      "Requirement already satisfied: idna==2.10 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (2.10)\n",
      "Requirement already satisfied: ipykernel==5.5.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (5.5.3)\n",
      "Requirement already satisfied: ipython==7.22.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (7.22.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (0.2.0)\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (7.6.3)\n",
      "Requirement already satisfied: jedi==0.18.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (0.18.0)\n",
      "Requirement already satisfied: Jinja2==2.11.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (2.11.3)\n",
      "Requirement already satisfied: joblib==1.0.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (1.0.1)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (3.2.0)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (1.0.0)\n",
      "Requirement already satisfied: jupyter-client==6.1.12 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (6.1.12)\n",
      "Requirement already satisfied: jupyter-console==6.4.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (6.4.0)\n",
      "Requirement already satisfied: jupyter-core==4.7.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (4.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (0.1.2)\n",
      "Requirement already satisfied: jupyterlab-widgets==1.0.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (1.1.1)\n",
      "Requirement already satisfied: mistune==0.8.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (0.8.4)\n",
      "Requirement already satisfied: murmurhash==1.0.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (1.0.5)\n",
      "Requirement already satisfied: nbclient==0.5.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert==6.0.7 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (6.0.7)\n",
      "Requirement already satisfied: nbformat==5.1.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (5.1.3)\n",
      "Requirement already satisfied: nest-asyncio==1.5.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (1.5.1)\n",
      "Requirement already satisfied: notebook==6.3.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (6.3.0)\n",
      "Requirement already satisfied: numpy==1.20.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (1.20.2)\n",
      "Requirement already satisfied: packaging==20.9 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (20.9)\n",
      "Requirement already satisfied: pandas==1.2.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (1.2.4)\n",
      "Requirement already satisfied: pandocfilters==1.4.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (1.4.3)\n",
      "Requirement already satisfied: parso==0.8.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (0.8.2)\n",
      "Requirement already satisfied: pathy==0.5.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (0.5.2)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (0.7.5)\n",
      "Requirement already satisfied: preshed==3.0.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (3.0.5)\n",
      "Requirement already satisfied: prometheus-client==0.10.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (0.10.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.18 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (3.0.18)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (0.7.0)\n",
      "Requirement already satisfied: pycparser==2.20 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (2.20)\n",
      "Requirement already satisfied: pydantic==1.7.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (1.7.3)\n",
      "Requirement already satisfied: Pygments==2.8.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (2.4.7)\n",
      "Requirement already satisfied: pyrsistent==0.17.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (0.17.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (2.8.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz==2021.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 58)) (2021.1)\n",
      "Requirement already satisfied: pyzmq==22.0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 59)) (22.0.3)\n",
      "Requirement already satisfied: qtconsole==5.0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 60)) (5.0.3)\n",
      "Requirement already satisfied: QtPy==1.9.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 61)) (1.9.0)\n",
      "Requirement already satisfied: requests==2.25.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 62)) (2.25.1)\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 63)) (0.24.2)\n",
      "Requirement already satisfied: scipy==1.6.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 64)) (1.6.3)\n",
      "Requirement already satisfied: Send2Trash==1.5.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 65)) (1.5.0)\n",
      "Requirement already satisfied: six==1.15.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 66)) (1.15.0)\n",
      "Requirement already satisfied: smart-open==3.0.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 67)) (3.0.0)\n",
      "Requirement already satisfied: spacy==3.0.6 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 68)) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy==3.0.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 69)) (3.0.5)\n",
      "Requirement already satisfied: srsly==2.4.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 70)) (2.4.1)\n",
      "Requirement already satisfied: terminado==0.9.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 71)) (0.9.4)\n",
      "Requirement already satisfied: testpath==0.4.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 72)) (0.4.4)\n",
      "Requirement already satisfied: thinc==8.0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 73)) (8.0.3)\n",
      "Requirement already satisfied: threadpoolctl==2.1.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 74)) (2.1.0)\n",
      "Requirement already satisfied: tornado==6.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 75)) (6.1)\n",
      "Requirement already satisfied: tqdm==4.60.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 76)) (4.60.0)\n",
      "Requirement already satisfied: traitlets==5.0.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 77)) (5.0.5)\n",
      "Requirement already satisfied: typer==0.3.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 78)) (0.3.2)\n",
      "Requirement already satisfied: urllib3==1.26.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 79)) (1.26.4)\n",
      "Requirement already satisfied: wasabi==0.8.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 80)) (0.8.2)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 81)) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 82)) (0.5.1)\n",
      "Requirement already satisfied: widgetsnbextension==3.5.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from -r requirements.txt (line 83)) (3.5.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from ipython==7.22.0->-r requirements.txt (line 19)) (52.0.0.post20210125)\n",
      "Collecting en-core-web-sm==3.0.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.60.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: jinja2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: setuptools in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/teofa/anaconda3/envs/nlu-sa/lib/python3.9/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f99102",
   "metadata": {},
   "source": [
    "In the next two code cells, we are importing every library we need as well as setting up any external dependency and loading the conll2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "equal-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary python libraries\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "increasing-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding to importables'path also the conll.py script, useful later on, and loading the conll2003 ds\n",
    "sys.path.insert(0, os.path.abspath('src/'))\n",
    "from conll import read_corpus_conll, evaluate\n",
    "conll2003 = read_corpus_conll('data/conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-married",
   "metadata": {},
   "source": [
    "### Task 1  \n",
    "In the first task students were required to evaluate the spaCy Named-Entity-Recognizer on the conll2003 dataset, with both **token-level** and **chunk-level** metrics.  \n",
    "  \n",
    "To this aim, two different strategies are shown in order to deal with the pre-defined tokens and labels of the conll2003 dataset:  \n",
    "1. the first implementation shows how pre-defined tokens can be **forced** inside a spaCy Doc representation;  \n",
    "2. the second implementation, on the other hand, **aligns** the original tokens and the ones automatically computed by spaCy. This allows for spaCy to use its own tokenization and possibly lead to better performances. Note, though, that also labels have been aligned accordingly.  \n",
    "  \n",
    "Afterwards, both the implementations are evaluated and the relative performances are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-diary",
   "metadata": {},
   "source": [
    "- ### Task 1.1  \n",
    "  \n",
    "In this section, we will focus on **token-level** metrics and how data have to be preprocessed according to the described strategies to suit our evaluation method. In the code cell below, some utility functions are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "conscious-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility fns for the first task\n",
    "def get_token(conll_tuple: tuple):\n",
    "    \"\"\"Returns the token text of a tuple from the conll2003 ds\"\"\"\n",
    "    decoded = conll_tuple[0].strip().split()\n",
    "    return decoded[0]\n",
    "\n",
    "def get_iob(conll_tuple: tuple):\n",
    "    \"\"\"Returns the iob tag of a tuple from the conll2003 ds\"\"\"\n",
    "    decoded = conll_tuple[0].strip().split()\n",
    "    return decoded[-1]\n",
    "\n",
    "def decode_ref(conll_tuple: tuple):\n",
    "    \"\"\"Decodes a tuple from the conll2003 dataset. Returns values as (token, iob)\"\"\"\n",
    "    return (get_token(conll_tuple), get_iob(conll_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d8003",
   "metadata": {},
   "source": [
    "### First method: using pre-defined tokens  \n",
    "In the below code cell, a simple whitespace tokenizer is defined. This will be then passed to a spaCy pipeline, overwriting its default tokenizer. With this tweak we will then be able to ensure spaCy uses the exact same tokens as defined in the conll2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "supreme-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a custom whitespace tokenizer\n",
    "class WhitespaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    Class for a basic whitespace tokenizer. When an instance is called, it splits a sentence string\n",
    "    into whitespace-separated tokens and embeds them into a Doc object to be returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        super(WhitespaceTokenizer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        spaces = [True] * len(words); spaces[-1] = False  # last word of a sentence doesn't need trailing space\n",
    "        return spacy.tokens.doc.Doc(self.vocab, words=words, spaces=spaces)\n",
    "    \n",
    "# initializing the default pipeline and substituting the tokenizer.\n",
    "# this will allow tokens provided by spacy and the original tokens from the conll2003 dataset to match.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29a2ff",
   "metadata": {},
   "source": [
    "After the pipeline has been initialized and its tokenizer has been overwritten, we can start collecting our references and our hypotheses, where:  \n",
    "- `refs` are composed of tuples of `(token, iob)` and represent each token together with its real IOB tag;  \n",
    "- `hyps` are composed of tuples `(token, iob)` where each token is paired with its IOB tag predicted by spaCy.  \n",
    "  \n",
    "To conclude the discussion about enforced tokenization, it is needed to say spaCy docs are initialized with strings made up of the different tokens of a sentence from conll2003, **whitespace-separated**. It follows naturally, then, that our `WhitespaceTokenizer` will simply split these sentence into the original tokens. Note that this could've been achieved in other ways, such as extracting the NER from a spaCy pipeline beforehand (with `nlp.get_pipe(\"ner\")`) in order to apply it to a Doc object manually initialized with the tokens retrieved from the current conll2003 sentence. \n",
    "  \n",
    "After each doc has been processed, and so the Named-Entity-Recognizer has been applied, the assigned labels need to be converted. In fact, the default IOB tags inside the conll2003 dataset also contain information concerning the categorization of entities, which spaCy stores inside the ad-hoc attribute `ent_type_` of each token. Also naming conventions are different, in spaCy's *PERSON* label maps to conll2003's *PER* one. Additionally, any category other than `[ORG, LOC, PERSON]` is mapped to `MISC`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "collaborative-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the refs and the hyps (tuples of token + iob tag wrt to NER task)\n",
    "refs, hyps = [], []\n",
    "\n",
    "# start looping through the sents of the dataset\n",
    "for sent in conll2003:\n",
    "    # get the tokens and the sentence\n",
    "    tokens = [get_token(tpl) for tpl in sent]\n",
    "    sent_str = \" \".join(tokens)\n",
    "    \n",
    "    # check this is not a DOCSTART line\n",
    "    if sent_str.startswith(\"-DOCSTART-\"): continue\n",
    "    \n",
    "    # if everything's fine, let's run the pipeline with our custom whitespace tokenizer.\n",
    "    # we are feeding the pipeline with whitespace-separated tokens, so this will turn out\n",
    "    # in using the tokens pre-defined by the conll2003 dataset.\n",
    "    doc = nlp(sent_str)\n",
    "        \n",
    "    # we must now convert spaCy NE labels to match the conll2003 notation for eval.\n",
    "    iobs_pred = []\n",
    "    for t in doc:\n",
    "        if t.ent_iob_ != 'O':\n",
    "            if t.ent_type_ in ['ORG', 'LOC']:\n",
    "                iobs_pred.append((t.text, f\"{t.ent_iob_}-{t.ent_type_}\"))\n",
    "            elif t.ent_type_ == 'PERSON':\n",
    "                iobs_pred.append((t.text, f\"{t.ent_iob_}-PER\"))\n",
    "            else:\n",
    "                iobs_pred.append((t.text, f\"{t.ent_iob_}-MISC\"))\n",
    "        else:\n",
    "            iobs_pred.append((t.text, 'O'))\n",
    "\n",
    "    # then append data for evaluation\n",
    "    refs.append([decode_ref(tpl) for tpl in sent])\n",
    "    hyps.append(iobs_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a49b6",
   "metadata": {},
   "source": [
    "When data has been preprocessed, **token-level** evaluation is performed with the help of scikit-learn's `classification_report`.  \n",
    "\n",
    "The function is simply fed with two lists containing the reference values and the hypotheses, equally indexed (i.e. `y_true[i]` and `y_pred[i]` both refer to the i-th token). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "permanent-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.52      0.02      0.03      1668\n",
      "      B-MISC       0.08      0.58      0.14       702\n",
      "       B-ORG       0.50      0.30      0.38      1661\n",
      "       B-PER       0.79      0.61      0.69      1617\n",
      "       I-LOC       0.41      0.08      0.13       257\n",
      "      I-MISC       0.05      0.41      0.08       216\n",
      "       I-ORG       0.42      0.52      0.46       835\n",
      "       I-PER       0.82      0.76      0.78      1156\n",
      "           O       0.94      0.86      0.90     38323\n",
      "\n",
      "    accuracy                           0.78     46435\n",
      "   macro avg       0.50      0.46      0.40     46435\n",
      "weighted avg       0.87      0.78      0.81     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when refs and hyps are ready, let's use sklearn to compute token-level metrics\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(refs)):\n",
    "    for j in range(len(refs[i])):\n",
    "        y_true.append(refs[i][j][-1])  # tuple is composed of (token, ref_tag) for further evaluation fn\n",
    "        y_pred.append(hyps[i][j][-1])  # tuple is composed of (token, pred_tag) for further evaluation fn\n",
    "\n",
    "# finally, compute and print the report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df33ec",
   "metadata": {},
   "source": [
    "### Second Method: token alignment  \n",
    "\n",
    "As mentioned in the intro of section one, the evaluation task has been addressed in order to look at both sides of the coin. In the code cells below, the implemented pipeline is the exact same one just described above. A significant difference, indeed, is represented by how tokens and labels are managed.\n",
    "\n",
    "Each sentence from spaCy is now tokenized by spaCy the default way, without enforcing any kind of tokenization. Afterwards, the obtained tokens and the source ones are **aligned** thanks to spaCy's `Alignment` class, allowing us to understand how the (possibly) different list of tokens map to each other. This serves as a mandatory step for the further label alignment: based on token alignment, also labels are preprocessed.  \n",
    "  \n",
    "Note, though, that a step is crucial here: when subsequent spaCy tokens map to one single input token, if this is labeled as a BOS we must pay attention. If we do not consider this situation, we will end up having sequences of B tags which, in reality, should represent only one entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "developed-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Alignment\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# build the refs and the hyps (tuples of token + iob tag wrt to NER task)\n",
    "refs_aligned, hyps_aligned = [], []\n",
    "\n",
    "# start looping through dataset sents\n",
    "for sent in conll2003:\n",
    "    # get the src tokens, the src iob tags and the src sentence\n",
    "    tokens = [get_token(tpl) for tpl in sent]\n",
    "    iobs = [get_iob(tpl) for tpl in sent]\n",
    "    sent_str = \" \".join(tokens)\n",
    "    \n",
    "    # check this is not a DOCSTART line\n",
    "    if sent_str.startswith(\"-DOCSTART-\"): continue\n",
    "    \n",
    "    # compute default spaCy tokens and get their inner text\n",
    "    doc = nlp(sent_str)\n",
    "    spacy_tokens = [t.text for t in doc]\n",
    "    \n",
    "    # align conll2003 tokens and spacy tokens\n",
    "    align = Alignment.from_strings(tokens, spacy_tokens)\n",
    "\n",
    "    # then, align the original labels to match the current tokens.\n",
    "    # If we encounter the same match twice (or more) in a row, \n",
    "    # we must convert its label if it is a BOS. Otherwise, we will\n",
    "    # end up having as groundtruth labels multiple contiguous B tags which\n",
    "    # do not have any meaning.\n",
    "    labels = []\n",
    "    last_match = None\n",
    "    for match in align.y2x.dataXd:\n",
    "        label = iobs[match]\n",
    "        if label.startswith(\"B-\") and last_match == match:\n",
    "            base, category = label.split(\"-\")\n",
    "            label = f\"I-{category}\"\n",
    "        last_match = match\n",
    "        labels.append(label)\n",
    "\n",
    "    # we must now convert spaCy NE labels to match the conll2003 notation for eval.\n",
    "    iobs_pred = []\n",
    "    for t in doc:\n",
    "        if t.ent_iob_ != 'O':\n",
    "            if t.ent_type_ in ['ORG', 'LOC']:\n",
    "                iobs_pred.append((t.text, f\"{t.ent_iob_}-{t.ent_type_}\"))\n",
    "            elif t.ent_type_ == 'PERSON':\n",
    "                iobs_pred.append((t.text, f\"{t.ent_iob_}-PER\"))\n",
    "            else:\n",
    "                iobs_pred.append((t.text, f\"{t.ent_iob_}-MISC\"))\n",
    "        else:\n",
    "            iobs_pred.append((t.text, 'O'))\n",
    "\n",
    "    # then append data for evaluation\n",
    "    refs_aligned.append([(token, y_true) for (token, y_true) in zip(spacy_tokens, labels)])\n",
    "    hyps_aligned.append(iobs_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "lined-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.48      0.02      0.03      1668\n",
      "      B-MISC       0.08      0.60      0.14       702\n",
      "       B-ORG       0.52      0.31      0.38      1661\n",
      "       B-PER       0.80      0.63      0.70      1617\n",
      "       I-LOC       0.33      0.07      0.12       276\n",
      "      I-MISC       0.04      0.31      0.07       322\n",
      "       I-ORG       0.41      0.52      0.46       876\n",
      "       I-PER       0.82      0.78      0.80      1210\n",
      "           O       0.95      0.85      0.90     40913\n",
      "\n",
      "    accuracy                           0.78     49245\n",
      "   macro avg       0.49      0.45      0.40     49245\n",
      "weighted avg       0.88      0.78      0.81     49245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# then, let's see the performance differences (if significant)\n",
    "# between the enforce tokenization and the aligned one.\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(refs_aligned)):\n",
    "    for j in range(len(refs_aligned[i])):\n",
    "        y_true.append(refs_aligned[i][j][-1])\n",
    "        y_pred.append(hyps_aligned[i][j][-1])\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-trainer",
   "metadata": {},
   "source": [
    "- ### Task 1.2  \n",
    "\n",
    "The second part of the first task of the assignment consisted in performing **chunk-level** evaluation, retrieving metrics such as precision, recall and f1-score. This has been implemented through the provided `conll.py`'s `evaluate` function, which suits perfectly. As data have already been prepared, the code cells below only display evaluation metrics for both the enforced tokenization output and the aligned one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "historic-cornwall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.761</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.078</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.137</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.448</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.339</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.246</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.280</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.481  0.016  0.030  1668\n",
       "PER    0.761  0.590  0.665  1617\n",
       "MISC   0.078  0.567  0.137   702\n",
       "ORG    0.448  0.272  0.339  1661\n",
       "total  0.246  0.324  0.280  5648"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get class-level scores, let's make use of conll.py\n",
    "results = evaluate(refs, hyps)\n",
    "pd.DataFrame().from_dict(results, orient='index').round(decimals=3)        # default WhiteSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "marked-meter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.131</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.464</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.346</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.244</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.280</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.450  0.016  0.031  1668\n",
       "PER    0.774  0.609  0.681  1617\n",
       "MISC   0.074  0.561  0.131   702\n",
       "ORG    0.464  0.276  0.346  1661\n",
       "total  0.244  0.330  0.280  5648"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_aligned = evaluate(refs_aligned, hyps_aligned)\n",
    "pd.DataFrame().from_dict(results_aligned, orient='index').round(decimals=3)  # aligned Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-dressing",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "For the second task, students were asked to perform grouping of entities as well as to analyze the most frequent NER combinations. As previously, some task-specific utility functions are defined in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "loaded-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_key(lst: list):\n",
    "    \"\"\"Transforms a list in a string format, useful to compute keys for frequency list generation.\"\"\"\n",
    "    return \"[{}]\".format(\", \".join(sorted(lst)))\n",
    "\n",
    "def nbest(d, n):\n",
    "    \"\"\"Returns first :param n: items of the frequency list :param d: (more frequent ones).\"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "def nworst(d, n):\n",
    "    \"\"\"Returns last :param n: items of the frequency list :param d: (less frequent ones).\"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1])[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358db91",
   "metadata": {},
   "source": [
    "Below the core functions of the task are defined:  \n",
    "\n",
    "`fn: group_sent` iterates through the `noun_chunks` of the entities of a sentence in order to understand which entity types are coupled. Then, also entities that do not appear together with other entities are taken into account, as they will single-handedly compose a group. \n",
    "\n",
    "When building groups, entity labels are **sorted**. This turns out to be significant when analyzing correlations and group frequencies. In this way, entities that appear with order `['A','B']` and `['B','A']` are both considered as the same entity group. For example, instances of groups `['PERSON', 'ORG']` and `['ORG', 'PERSON']` will account for `['ORG', 'PERSON']` at the end.  \n",
    "**This is merely a design choice**.  \n",
    "In my opinion, this is more robust than considering `['ORG', 'PERSON']` and `['PERSON', 'ORG']` as different \"events\" to be tracked, for instances, and enables us to capture linguistic expressions such as *Apple's Steve Jobs* and *Steve Jobs of Apple* which refer to the same real world entities.\n",
    "\n",
    "`fn: fl` builds a frequency list of groups and is meant to be fed with a list containing outputs of `fn: group_sent`. By iterating through the input list, it builds a Python dictionary where the keys are the representations of entity groups and the values are the counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "close-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sent(doc: spacy.tokens.doc.Doc):\n",
    "    \"\"\"\n",
    "    Process a spacy Doc to get Named-Entity groups that appear together.\n",
    "    At first, groups are built based on which entities belong to the same noun chunks.\n",
    "    Afterwards, also entities that are not coupled with any other entity in a noun chunk are \n",
    "    taken into account as a mocked group with 1 NE.  \n",
    "    :return: list of lists, where each inner list is made up of entity labels that appear together.\n",
    "    \"\"\"\n",
    "    stored_ents = []\n",
    "    ent_groups = []\n",
    "\n",
    "    # add all groups that appear in noun chunks, taking care of avoiding repetitions\n",
    "    for nc in doc.noun_chunks:\n",
    "        to_store = []\n",
    "        for ent in nc.ents:\n",
    "            if ent not in stored_ents:\n",
    "                to_store.append(ent.label_)\n",
    "                stored_ents.append(ent)\n",
    "        if len(to_store) > 0:\n",
    "            ent_groups.append(sorted(to_store))          \n",
    "    \n",
    "    # take also into account entities that do not appear in noun chunks\n",
    "    # and count them as groups composed by only 1 NE\n",
    "    for ent in doc.ents:\n",
    "        if ent not in stored_ents:\n",
    "            ent_groups.append([ent.label_])\n",
    "    \n",
    "    return ent_groups\n",
    "\n",
    "def fl(groups):\n",
    "    \"\"\"\n",
    "    Build the frequency list of groups. Each key is made up of a string representation\n",
    "    of the group and occurrences and the respective values would represent occurences\n",
    "    of that particular Named-Entities combination.\n",
    "    \n",
    "    :param groups: list-of-lists-of-lists, where: \n",
    "    - Outermost list indexes sentences;  \n",
    "    (groups[i] will be a list-of-lists, groups of sent. i)\n",
    "    \n",
    "    - Intermediate lists index groups of a sentence; \n",
    "    (groups[i][j] will be a list, labels of group j of sent. i)\n",
    "    \n",
    "    - Innermost lists index entity labels of a group of a sentence. \n",
    "    (groups[i][j][k] will be label k of group j of sent. i)\n",
    "    \n",
    "    :return fdict: frequency list.\n",
    "    \"\"\"\n",
    "    fdict = {}\n",
    "    # scan each sentence\n",
    "    for sent_groups in groups:\n",
    "        # scan each group within a sentence\n",
    "        for group in sent_groups:  # ATTENTION: 'group' is a list and can be empty\n",
    "            if len(group) == 0: continue\n",
    "            key = encode_key(group)\n",
    "            # create the dict with default counts if needed\n",
    "            if key not in fdict.keys():\n",
    "                fdict[key] = 0\n",
    "            # update the counts for the given group\n",
    "            fdict[key] += 1\n",
    "    return fdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b15c0",
   "metadata": {},
   "source": [
    "Groups are then built for the whole conll2003 dataset, and the frequency list is computed. The 10 most frequent and least frequent entity groups are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "peripheral-robin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 most frequent NER combinations\n",
      "[CARDINAL] 1624\n",
      "[GPE] 1255\n",
      "[PERSON] 1074\n",
      "[DATE] 997\n",
      "[ORG] 873\n",
      "[NORP] 293\n",
      "[MONEY] 147\n",
      "[ORDINAL] 111\n",
      "[TIME] 83\n",
      "[PERCENT] 81\n",
      "\n",
      "10 least frequent NER combinations\n",
      "[DATE, ORDINAL] 1\n",
      "[GPE, ORDINAL, ORG] 1\n",
      "[ORG, QUANTITY] 1\n",
      "[CARDINAL, GPE, GPE] 1\n",
      "[DATE, LOC] 1\n",
      "[DATE, FAC] 1\n",
      "[CARDINAL, CARDINAL, NORP] 1\n",
      "[LOC, ORDINAL] 1\n",
      "[MONEY, ORG] 1\n",
      "[LOC, NORP] 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# initialize the list where groups per sentence will be put\n",
    "groups = []\n",
    "\n",
    "# process the whole ds\n",
    "for sent in conll2003:\n",
    "    # same tasks performed also in previous cells\n",
    "    tokens = [get_token(tpl) for tpl in sent]\n",
    "    sent_str = \" \".join(tokens)\n",
    "    if sent_str.startswith(\"-DOCSTART-\"): continue\n",
    "    \n",
    "    # group entities in the current sentence and store groups\n",
    "    doc = nlp(sent_str)\n",
    "    sent_groups = group_sent(doc)\n",
    "    groups.append(sent_groups)\n",
    "\n",
    "# Analyze the groups in terms of most frequent combinations (i.e. NER types that go together)\n",
    "frequency_list = fl(groups)\n",
    "\n",
    "print(\"\\n10 most frequent NER combinations\")\n",
    "out = nbest(frequency_list, n=10)\n",
    "[print(k,v) for k,v in out.items()]\n",
    "\n",
    "print(\"\\n10 least frequent NER combinations\")\n",
    "out = nworst(frequency_list, n=10)\n",
    "[print(k,v) for k,v in out.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-earthquake",
   "metadata": {},
   "source": [
    "### Task 3  \n",
    "\n",
    "For task three, the goal was to perform a common post-processing task, that is (trying to) fix segmentation errors based on the `compound` dependency relation. This is achieved by means of `extend`, a **recursive function**. The behaviour of the function is rather simple:  \n",
    "- if an entity span has no tokens with the compound dependency relation, no extension is performed;  \n",
    "- otherwise, extension is performed by including the head of compounds inside the current span as long as brand new compounds are introduced in the span (i.e. recursion keeps going until the set difference between the new compounds and the previous compounds is zero). This allows to \"climb\" the dependency tree up to all reachable compounds from the original span.  \n",
    "\n",
    "In the code cell below, what's needed to extend entities is defined and the implementation is then tested against the first 20 sentences of the conll2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "respected-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .'\n",
      "Original entities:  [CHINA]\n",
      "Extended entities:  [CHINA]\n",
      "\n",
      "\n",
      "\n",
      "'Nadim Ladki'\n",
      "Original entities:  [Nadim Ladki]\n",
      "Extended entities:  [Nadim Ladki]\n",
      "\n",
      "\n",
      "\n",
      "'AL-AIN , United Arab Emirates 1996-12-06'\n",
      "Original entities:  [AL-AIN, United Arab Emirates, 1996-12-06]\n",
      "Extended entities:  [AL-AIN, United Arab Emirates, 1996-12-06]\n",
      "\n",
      "\n",
      "\n",
      "'Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .'\n",
      "Original entities:  [Japan, their Asian Cup, Syria, Group C, Friday]\n",
      "Extended entities:  [Japan, their Asian Cup title, Syria, Group C championship match, Friday]\n",
      "\n",
      "\n",
      "\n",
      "'But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .'\n",
      "Original entities:  [China, second, 2, Uzbekistan]\n",
      "Extended entities:  [China, second, 2, Uzbekistan]\n",
      "\n",
      "\n",
      "\n",
      "'China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .'\n",
      "Original entities:  [China, the 78th minute, Uzbek, Igor Shkvyrin, Chinese]\n",
      "Extended entities:  [China, the 78th minute, Uzbek striker Igor Shkvyrin, Igor Shkvyrin, Chinese]\n",
      "\n",
      "\n",
      "\n",
      "'Oleg Shatskiku made sure of the win in injury time , hitting an unstoppable left foot shot from just outside the area .'\n",
      "Original entities:  [Oleg Shatskiku]\n",
      "Extended entities:  [Oleg Shatskiku]\n",
      "\n",
      "\n",
      "\n",
      "'The former Soviet republic was playing in an Asian Cup finals tie for the first time .'\n",
      "Original entities:  [Soviet, Asian Cup, first]\n",
      "Extended entities:  [Soviet, Asian Cup finals tie, first]\n",
      "\n",
      "\n",
      "\n",
      "'Despite winning the Asian Games title two years ago , Uzbekistan are in the finals as outsiders .'\n",
      "Original entities:  [two years ago, Uzbekistan]\n",
      "Extended entities:  [two years ago, Uzbekistan]\n",
      "\n",
      "\n",
      "\n",
      "'Two goals from defensive errors in the last six minutes allowed Japan to come from behind and collect all three points from their opening meeting against Syria .'\n",
      "Original entities:  [Two, the last six minutes, Japan, three, Syria]\n",
      "Extended entities:  [Two, the last six minutes, Japan, three, Syria]\n",
      "\n",
      "\n",
      "\n",
      "'Takuya Takagi scored the winner in the 88th minute , rising to head a Hiroshige Yanagimoto cross towards the Syrian goal which goalkeeper Salem Bitar appeared to have covered but then allowed to slip into the net .'\n",
      "Original entities:  [Takuya Takagi, the 88th minute, Hiroshige Yanagimoto, Syrian, Salem, Bitar]\n",
      "Extended entities:  [Takuya Takagi, the 88th minute, Hiroshige Yanagimoto cross, Syrian, Salem Bitar, Bitar]\n",
      "\n",
      "\n",
      "\n",
      "'It was the second costly blunder by Syria in four minutes .'\n",
      "Original entities:  [second, Syria, four minutes]\n",
      "Extended entities:  [second, Syria, four minutes]\n",
      "\n",
      "\n",
      "\n",
      "'Defender Hassan Abbas rose to intercept a long ball into the area in the 84th minute but only managed to divert it into the top corner of Bitar 's goal .'\n",
      "Original entities:  [Defender Hassan Abbas, Bitar]\n",
      "Extended entities:  [Defender Hassan Abbas, Bitar]\n",
      "\n",
      "\n",
      "\n",
      "'Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute .'\n",
      "Original entities:  [Nader Jokhadar, Syria]\n",
      "Extended entities:  [Nader Jokhadar, Syria]\n",
      "\n",
      "\n",
      "\n",
      "'Japan then laid siege to the Syrian penalty area for most of the game but rarely breached the Syrian defence .'\n",
      "Original entities:  [Japan, Syrian, Syrian]\n",
      "Extended entities:  [Japan, Syrian, Syrian]\n",
      "\n",
      "\n",
      "\n",
      "'Bitar pulled off fine saves whenever they did .'\n",
      "Original entities:  [Bitar]\n",
      "Extended entities:  [Bitar]\n",
      "\n",
      "\n",
      "\n",
      "'Japan coach Shu Kamo said : ' ' The Syrian own goal proved lucky for us .'\n",
      "Original entities:  [Japan, Shu Kamo, Syrian]\n",
      "Extended entities:  [Japan coach Shu Kamo, Shu Kamo, Syrian]\n",
      "\n",
      "\n",
      "\n",
      "'The Syrians scored early and then played defensively and adopted long balls which made it hard for us . ''\n",
      "Original entities:  [Syrians]\n",
      "Extended entities:  [Syrians]\n",
      "\n",
      "\n",
      "\n",
      "'''\n",
      "Original entities:  []\n",
      "Extended entities:  []\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compounds(ent: spacy.tokens.span.Span):\n",
    "    \"\"\"Get all tokens with the compound dependency relation inside :param ent:.\"\"\"\n",
    "    comps = []\n",
    "    for t in ent:\n",
    "        if t.dep_==\"compound\":\n",
    "            comps.append(t)\n",
    "    return comps\n",
    "\n",
    "def extend_entity_span(ent: spacy.tokens.span.Span):\n",
    "    \"\"\"\n",
    "    Extend the span of an entity based on its compounds. Heads of compound (i.e. nouns that\n",
    "    are qualified by some token in the entity) will be added to the entity, extending it.\n",
    "    :param ent: input entity to be extended\n",
    "    :return:    extended span of the entity (spaCy span)\n",
    "    \"\"\"\n",
    "    ent_ext = []\n",
    "    # checking noun compounds for each token\n",
    "    for t in ent:\n",
    "        # appending the current token to the extended entity\n",
    "        ent_ext.append(t)\n",
    "        # if the token is a compound wrt sth outside of the entity, let's add the head too\n",
    "        if t.dep_ == \"compound\" and t.head not in ent and t.head not in ent_ext:\n",
    "            ent_ext.append(t.head)\n",
    "            \n",
    "    # ensure tokens are in the original sentence order and return\n",
    "    ent_ext = sorted(ent_ext, key=lambda x:x.i)\n",
    "    ent_ext = ent.doc[ent_ext[0].i:ent_ext[-1].i+1]  # as a Span object\n",
    "    return ent_ext\n",
    "\n",
    "def extend(ent: spacy.tokens.span.Span, prev_comps=set()):\n",
    "    \"\"\"\n",
    "    Recursively extend the span of an entity based on the 'compound' dependency relation.\n",
    "    Recursion stops when no new compounds are found.\n",
    "    \"\"\"\n",
    "    # perform set difference between the current compounds and the compounds of the \n",
    "    # previous recursive call.\n",
    "    current_comps = set(compounds(ent))\n",
    "    diff = current_comps - prev_comps\n",
    "    \n",
    "    # base case, if no new compounds have been found, return.\n",
    "    if len(diff) == 0:\n",
    "        return ent\n",
    "    \n",
    "    # otherwise, perform a first-level extension of the span and proceed with recursion.\n",
    "    ent = extend_entity_span(ent)\n",
    "    return extend(ent, current_comps)\n",
    "\n",
    "# test out the implementation on a slice of the conll2003 dataset\n",
    "for sent in conll2003[:20]:\n",
    "    tokens = [get_token(tpl) for tpl in sent]\n",
    "    sent_str = \" \".join(tokens)\n",
    "    if \" \".join(tokens).startswith(\"-DOCSTART-\"): continue\n",
    "    doc = nlp(sent_str)\n",
    "    original = [ent for ent in doc.ents]\n",
    "    extended = [extend(ent) for ent in doc.ents]\n",
    "    print(\"'{}'\".format(sent_str))\n",
    "    print(\"Original entities: \", original)\n",
    "    print(\"Extended entities: \", extended)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df7185",
   "metadata": {},
   "source": [
    "As we can see, extension does its job.\n",
    "\n",
    "For instance, in sentence: \"*China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .*\" initially `Uzbek` and `Igor Shkvyrin` were considered different entities, while performing extension allowed us to merge them into a new one.\n",
    "\n",
    "Indeed, there is still something we can do. It's true that entities have been extended in this way, but we haven't **pruned** useless entities accordingly. Looking at the same sentence, we may notice that if the named entity `Uzbek striker Igor Shkvyrin` appears, it doesn't make sense to also have `Igor Shkvyrin` as a separate, different NE. So, we may think of getting rid of it.\n",
    "\n",
    "For this reason, the following (and last) code cell implements a filtering function, named **filter_ents**. Given a list of entities, it checks whether some entity span is fully included into some other one and can, thus, be safely pruned. Span inclusion is checked leveraging the `i` attribute of spaCy tokens.\n",
    "\n",
    "Then, the implementation will be tested on the same slice of conll2003. As you will see, redundant entities will disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45810a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ents(ents: list):\n",
    "    \"\"\"\n",
    "    Process all extended entities of a sentence (:param ents:). If an entity span is found\n",
    "    to be a sub-span of another entity, this implies that two original named-entity where in reality\n",
    "    linked by a compound dependency, so the extension has merged them. Thus, the sub-span can be removed\n",
    "    as it is redundant and less informative.\n",
    "    \n",
    "    :param ents: list of extended entities to be checked;\n",
    "    :return:     list of extended entities, where fully included sub-spans of other entities\n",
    "                 in :param ents: have been pruned. \n",
    "    \"\"\"\n",
    "    extended_filtered = []\n",
    "    for ext_ent in ents:\n",
    "        # perform type check for each item of the provided list\n",
    "        assert type(ext_ent) == spacy.tokens.span.Span, \\\n",
    "        \"(fn: filter_ents) each item of argument 'ents' must be a Span obj.\"\n",
    "        \n",
    "        keep = True\n",
    "        start, stop = ext_ent[0].i, ext_ent[-1].i\n",
    "        for ext_match in ents:\n",
    "            match_start, match_stop = ext_match[0].i, ext_match[-1].i       \n",
    "            # now, check if the span intersection is not empty\n",
    "            # in order, the following types of inclusion are checked:\n",
    "            # - strict inclusion;\n",
    "            # - non-strict left-inclusion;\n",
    "            # - non-strict right-inclusion.\n",
    "            if (match_start < start and stop < match_stop) or \\\n",
    "            (match_start <= start and stop < match_stop) or \\\n",
    "            (match_start < start and stop <= match_stop): \n",
    "                keep = False\n",
    "                break\n",
    "        \n",
    "        if keep: extended_filtered.append(ext_ent)\n",
    "    return extended_filtered\n",
    "\n",
    "# test out the implementation on a slice of the conll2003 dataset\n",
    "for sent in conll2003[:20]:\n",
    "    tokens = [get_token(tpl) for tpl in sent]\n",
    "    sent_str = \" \".join(tokens)\n",
    "    if \" \".join(tokens).startswith(\"-DOCSTART-\"): continue\n",
    "    doc = nlp(sent_str)\n",
    "    original = [ent for ent in doc.ents]\n",
    "    extended = [extend(ent) for ent in doc.ents]\n",
    "    filtered = filter_ents(extended)\n",
    "    print(\"'{}'\".format(sent_str))\n",
    "    print(\"Original entities: \", original)\n",
    "    print(\"Extended entities: \", extended)\n",
    "    print(\"Filtered entities: \", filtered)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0027d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
